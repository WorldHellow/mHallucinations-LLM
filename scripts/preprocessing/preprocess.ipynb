{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-entity': 1, 'I-relation': 2, 'I-invented': 3, 'I-contradictory': 4, 'I-unverifiable': 5, 'I-subjective': 6, 'O': 0, 'ignore': -100}\n",
      "{1: 'I-entity', 2: 'I-relation', 3: 'I-invented', 4: 'I-contradictory', 5: 'I-unverifiable', 6: 'I-subjective', 0: 'O', -100: 'ignore'}\n"
     ]
    }
   ],
   "source": [
    "# Define the categories and create mappings\n",
    "categories = [\"entity\", \"relation\", \"invented\", \"contradictory\", \"unverifiable\", \"subjective\"]\n",
    "label2id = {f'I-{category}': idx+1 for idx, category in enumerate(categories)}\n",
    "label2id[\"O\"] = 0\n",
    "label2id['ignore'] = -100\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_all_v1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 654066/654066 [01:32<00:00, 7094.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved to train_all_v1.json\n",
      "Loading test_all_v1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 8700/8700 [00:01<00:00, 7947.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved to test_all_v1.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import jieba\n",
    "from janome.tokenizer import Tokenizer as JapaneseTokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def process_messages(data):\n",
    "    try:\n",
    "        # Extracting the language\n",
    "        system_content = data['messages'][2]['content']\n",
    "        language = system_content.split()[7]\n",
    "\n",
    "        # Extracting references\n",
    "        user_content = data['messages'][0]['content']\n",
    "        if '$$$' in user_content:\n",
    "            references_split = user_content.split('$$$')[1:-1]\n",
    "        elif '$ $ $' in user_content:\n",
    "            references_split = user_content.split('$ $ $')[1:-1]\n",
    "        else:\n",
    "            references_split = []\n",
    "        references = \" \".join(references_split)\n",
    "\n",
    "        # Extracting text without labels from assistant content\n",
    "        assistant_content = data['messages'][1]['content']\n",
    "        text = re.sub(r'<[^>]*>', '', assistant_content)\n",
    "\n",
    "        # Getting the assistant content as it is\n",
    "        tagged_sequences = assistant_content\n",
    "\n",
    "        return references, text, tagged_sequences, language\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing messages: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def find_start_index(tokens, tagged_tokens):\n",
    "    for i in range(len(tokens) - len(tagged_tokens) + 1):\n",
    "        if tokens[i:i+len(tagged_tokens)] == tagged_tokens:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def bio_annotation(text, tagged_sequences, language):\n",
    "    if language in ['chinese', 'cantonese']:\n",
    "        tokens = list(jieba.cut(text))\n",
    "    elif language == 'japanese':\n",
    "        tokens = [token.surface for token in japanese_tokenizer.tokenize(text)]\n",
    "    elif language =='thai':\n",
    "        tokens = word_tokenize(text, engine='newmm') \n",
    "    else:\n",
    "        tokens = text.split()\n",
    "    \n",
    "    tags = ['O'] * len(tokens)\n",
    "    \n",
    "    # Define the categories\n",
    "    categories = [\"entity\", \"relation\", \"invented\", \"contradictory\", \"unverifiable\", \"subjective\"]\n",
    "    \n",
    "    # Process each tagged sequence\n",
    "    for category in categories:\n",
    "        pattern = f\"<{category}>(.*?)</{category}>\"\n",
    "        matches = re.finditer(pattern, tagged_sequences, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            tagged_text = match.group(1).strip()\n",
    "            if language in ['chinese', 'cantonese']:\n",
    "                tagged_tokens = list(jieba.cut(tagged_text))\n",
    "            elif language == 'japanese':\n",
    "                tagged_tokens = [token.surface for token in japanese_tokenizer.tokenize(tagged_text)]\n",
    "            elif language =='thai':\n",
    "                tagged_tokens = word_tokenize(tagged_text, engine='newmm') \n",
    "            else:\n",
    "                tagged_tokens = tagged_text.split()\n",
    "            \n",
    "            # Find the start index of the tagged sequence in the token list\n",
    "            start_idx = find_start_index(tokens, tagged_tokens)\n",
    "            \n",
    "            if start_idx != -1:\n",
    "                # Tag the tokens\n",
    "                for i in range(len(tagged_tokens)):\n",
    "                    if start_idx + i < len(tags):\n",
    "                        tags[start_idx + i] = f'I-{category}'\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def concatenate_references_and_text(references, text, lang):\n",
    "    if lang in ['chinese', 'japanese', 'cantonese']:\n",
    "        return references + 'reservedspecialtoken' + text\n",
    "    else:\n",
    "        return references + ' <|reserved_special_token_200|> ' + text\n",
    "\n",
    "def create_tag_input(input_text, references, text, tags, language):\n",
    "    if language in ['chinese', 'cantonese']:\n",
    "        input_tokens = list(jieba.cut(input_text))\n",
    "        text_tokens = list(jieba.cut(text))\n",
    "    elif language == 'japanese':\n",
    "        input_tokens = [token.surface for token in japanese_tokenizer.tokenize(input_text)]\n",
    "        text_tokens = [token.surface for token in japanese_tokenizer.tokenize(text)]\n",
    "    elif language =='thai':\n",
    "        input_tokens = word_tokenize(input_text, engine='newmm') \n",
    "        text_tokens = word_tokenize(text, engine='newmm') \n",
    "    else:\n",
    "        input_tokens = input_text.split()\n",
    "        text_tokens = text.split()\n",
    "    \n",
    "    if len(text_tokens) != len(tags):\n",
    "        raise ValueError(\"The number of tokens in the text and tags do not match.\")\n",
    "    \n",
    "    tags_for_input = ['ignore'] * len(input_tokens)\n",
    "    \n",
    "    if language == 'chinese':\n",
    "        references_tokens = list(jieba.cut(references))\n",
    "    elif language == 'japanese':\n",
    "        references_tokens = [token.surface for token in japanese_tokenizer.tokenize(references)]\n",
    "    elif language =='thai':\n",
    "        references_tokens  = word_tokenize(references, engine='newmm') \n",
    "    else:\n",
    "        references_tokens = references.split()\n",
    "    \n",
    "    text_start_index = len(references_tokens)\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        if text_start_index + i < len(tags_for_input):\n",
    "            tags_for_input[text_start_index + i] = tag\n",
    "    \n",
    "    return tags_for_input\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    # Read the dataset\n",
    "    print(f'Loading {output_file}')\n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    # Process each entry in the dataset\n",
    "    for entry in tqdm(data,desc='Processing'):\n",
    "        references, text, tagged_sequences, language = process_messages(entry)\n",
    "        if references is not None and text is not None and tagged_sequences is not None and language is not None:\n",
    "            tags = bio_annotation(text, tagged_sequences, language)\n",
    "            input_text = concatenate_references_and_text(references, text, language)\n",
    "            tag_input = create_tag_input(input_text, references, text, tags, language)\n",
    "            processed_data.append({\n",
    "                'references': references,\n",
    "                'text': text,\n",
    "                'tagged_sequences': tagged_sequences,\n",
    "                'language': language,\n",
    "                'tags': tags,\n",
    "                'input': input_text,\n",
    "                'tag_input': tag_input\n",
    "            })\n",
    "\n",
    "    # Save the processed data to a JSON file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(processed_data, outfile, indent=4)\n",
    "\n",
    "    print(\"Dataset has been saved to\", output_file)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = \"/p/project/westai0015/code_julich/hallucinations/datasets/all-ref/combined_train.json\"\n",
    "    test_file = \"/p/project/westai0015/code_julich/hallucinations/datasets/all-ref/combined_test.json\" \n",
    "    output_train_file = 'train_all_v1.json'\n",
    "    output_test_file = 'test_all_v1.json'\n",
    "    main(train_file, output_train_file)\n",
    "    main(test_file, output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-entity': 1, 'I-relation': 2, 'I-invented': 3, 'I-contradictory': 4, 'I-unverifiable': 5, 'I-subjective': 6, 'O': 0, 'ignore': -100}\n",
      "{1: 'I-entity', 2: 'I-relation', 3: 'I-invented', 4: 'I-contradictory', 5: 'I-unverifiable', 6: 'I-subjective', 0: 'O', -100: 'ignore'}\n",
      "Loading File test_all_v1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 8700it [00:00, 16841.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data converted and saved successfully.\n",
      "Invalid indices saved to invalid_indices.json.\n",
      "Loading File train_all_v1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 654066it [01:01, 10696.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data converted and saved successfully.\n",
      "Invalid indices saved to invalid_indices.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from janome.tokenizer import Tokenizer as JapaneseTokenizer\n",
    "# Define the categories and create mappings\n",
    "# Define the categories and create mappings\n",
    "categories = [\"entity\", \"relation\", \"invented\", \"contradictory\", \"unverifiable\", \"subjective\"]\n",
    "label2id = {f'I-{category}': idx+1 for idx, category in enumerate(categories)}\n",
    "label2id[\"O\"] = 0\n",
    "label2id['ignore'] = -100\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "\n",
    "def tags_to_ids(tags):\n",
    "    return [label2id[tag] for tag in tags]\n",
    "\n",
    "def custom_jieba_cut(text):\n",
    "    # Ensure the special token is handled as a single token\n",
    "    #text = text.replace(' <|reserved_special_token_200|> ', ' reservedspecialtoken ')\n",
    "    tokens = list(jieba.cut(text))\n",
    "    # Restore the special token to its original form\n",
    "    tokens = ['<|reserved_special_token_200|>' if token == 'reservedspecialtoken' else token for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def create_training_data(entry, index):\n",
    "    if entry['language'] in ['chinese', 'cantonese']:\n",
    "        tokens = custom_jieba_cut(entry['input'])\n",
    "    elif entry['language'] == 'thai':\n",
    "        tokens  = word_tokenize(entry['input'], engine='newmm') \n",
    "    elif entry['language'] == 'japanese':\n",
    "        tokens = [token.surface for token in japanese_tokenizer.tokenize(entry['input'])]\n",
    "    else:\n",
    "        tokens = entry['input'].split()\n",
    "    \n",
    "    tags = entry['tag_input']\n",
    "    ner_tags = tags_to_ids(tags)\n",
    "    \n",
    "    return {\n",
    "        'id': str(index),\n",
    "        'tokens': tokens,\n",
    "        'labels': ner_tags,\n",
    "        'tags': tags,\n",
    "        'language': entry['language']\n",
    "    }\n",
    "\n",
    "def convert_to_training_format(input_file, output_file):\n",
    "    # Load the processed dataset\n",
    "    print(f'Loading File {input_file}')\n",
    "    with open(input_file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "\n",
    "    valid_data = []\n",
    "    invalid_indices = []\n",
    "    all_languages = []\n",
    "\n",
    "    for index, entry in tqdm(enumerate(data), desc='Processing'):\n",
    "        try:\n",
    "            all_languages.append(entry['language'])\n",
    "            training_entry = create_training_data(entry, index)\n",
    "            if len(training_entry['tokens']) == len(training_entry['labels']):\n",
    "                valid_data.append(training_entry)\n",
    "            else:\n",
    "                print(f'Invalid sampled detected at index {index}')\n",
    "                invalid_indices.append(index)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry {index}: {e}\")\n",
    "            invalid_indices.append(index)\n",
    "\n",
    "    # Save valid data to a new JSON file\n",
    "    with open('all_languages.txt', 'w') as f:\n",
    "        f.write(str(set(all_languages)))\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(valid_data, outfile, indent=4)\n",
    "\n",
    "    # Save invalid indices to a separate file\n",
    "    with open('invalid_indices.json', 'w') as f:\n",
    "        json.dump(invalid_indices, f, indent=4)\n",
    "\n",
    "    print(\"Training data converted and saved successfully.\")\n",
    "    print(\"Invalid indices saved to invalid_indices.json.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_train_file = 'train_all_v1.json'\n",
    "    input_test_file = 'test_all_v1.json'  # replace with your desired output file name\n",
    "    output_train_file = 'train_all_final.json'\n",
    "    output_test_file = 'test_all_final.json'\n",
    "    convert_to_training_format(input_test_file, output_test_file)\n",
    "    convert_to_training_format(input_train_file, output_train_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -100]\n",
      "All entries have matching lengths of tokens and ner_tags.\n"
     ]
    }
   ],
   "source": [
    "def validate_training_data(input_file):\n",
    "    # Load the training dataset\n",
    "    with open(input_file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    invalid_entries = []\n",
    "\n",
    "    for index, entry in enumerate(data):\n",
    "        tokens = entry['tokens']\n",
    "        ner_tags = entry['labels']\n",
    "        \n",
    "        if len(tokens) != len(ner_tags):\n",
    "            invalid_entries.append({\n",
    "                'id': entry['id'],\n",
    "                'tokens_length': len(tokens),\n",
    "                'labels_length': len(ner_tags)\n",
    "            })\n",
    "    print(ner_tags)\n",
    "    if invalid_entries:\n",
    "        print(f\"Found {len(invalid_entries)} invalid entries with mismatched lengths of tokens and ner_tags.\")\n",
    "        # Save invalid entries to a JSON file for further inspection\n",
    "        with open('invalid_entries.json', 'w') as outfile:\n",
    "            json.dump(invalid_entries, outfile, indent=4)\n",
    "        print(\"Invalid entries saved to invalid_entries.json.\")\n",
    "    else:\n",
    "        print(\"All entries have matching lengths of tokens and ner_tags.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'test_all_final.json'  # replace with your input file name\n",
    "    validate_training_data(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
